# mystars
个人的github stars，主要是机器学习、深度学习、NLP、GNN、大数据等内容。

### 机器学习
MNN 一个轻量级的深度神经网络推理引擎

Tencent/TNN 移动端高性能、轻量级推理框架，同时拥有跨平台、高性能、模型压缩、代码裁剪等众多突出优势

microsoft/nnfusion 灵活高效的深度神经网络（DNN）编译器，可从DNN模型描述生成高性能的可执行文件。

apache/incubator-tvm 用于深度学习系统的编译器堆栈。它旨在缩小以生产力为中心的深度学习框架与以性能和效率为重点的硬件后端之间的差距。TVM与深度学习框架一起使用，以提供对不同后端的端到端编译

geohot/tinygrad 一个不到1000行的深度学习框架，麻雀虽小，但五脏俱全，这个深度学习框架使用起来和PyTorch类似

karpathy/micrograd 一个微型标量自动求导引擎，类似PyTorch API的神经网络库

Weights and Biases 使用 W&B 组织和分析机器学习实验

ALiPy 一个基于Python实现的主动学习工具包

Nevergrad 无梯度优化平台

combo 用于机器学习模型组合的 Python 工具箱

google/trax 代码更清晰的神经网络代码库

Oneflow-Inc/oneflow OneFlow是一个以性能为中心的开源深度学习框架。

jonasrauber/eagerpy 编写与PyTorch，TensorFlow，JAX和NumPy本地兼容的代码

rushter/MLAlgorithms 机器学习算法

MLEveryday/100-Days-Of-ML-Code

csuldw/MachineLearning

luwill/machine-learning-code-writing

CDCS 中国数据竞赛优胜解集锦

mlpack/mlpack 快速、灵活的机器学习库

tensorflow/ranking  排名学习在TensorFlow中

scikit-survival 生存分析

leibinghe/GAAL-based-outlier-detection 基于盖尔的异常检测

yzhao062/pyod 异常检测

lavender28/Credit-Card-Score 申请信用评分卡模型

modin-project/modin 通过更改一行代码来扩展加速pandas 

vaexio/vaex 适用于Python的核外DataFrame，以每秒十亿行的速度可视化和探索大型表格数据

cupy/cupy NumPy-like API accelerated with CUDA https://cupy.chainer.org

pythran 将 Python 代码转成 C++ 代码执行 一个 AOT (Ahead-Of-Time - 预先编译) 编译器，大幅度提升性能。

RAPIDS Open GPU Data Science http://rapids.ai

cudf cuDF - GPU DataFrame Library

cuml cuML - RAPIDS Machine Learning Library

cugraph cuGraph - RAPIDS Graph Analytics Library

cusignal cuSignal - RAPIDS Signal Processing Library

AtsushiSakai/PythonRobotics 机器人算法

cuML GPU 机器学习算法

SQLFlow 连接 SQL 引擎的桥接，与机器学习工具包连接

FeatureLabs/featuretools

esa/pagmo2 大规模并行优化的科学库 生物启发式算法和进化算法

geatpy-dev/geatpy 高性能遗传进化算法工具箱

guofei9987/scikit-opt 强大的启发式算法Python模块  遗传算法 粒子群优化 模拟退火 蚁群算法 免疫算法 人工鱼群算法

kingfengji/gcForest Deep forest

interpretml/interpret 训练可解释的机器学习模型和解释黑匣子系统

alexmojaki/heartrate 调试 Python程序执行的简单实时可视化

google-research/mixmatch 集成了自洽正则化的超强半监督学习 MixMatch 

bojone/keras_recompute 通过重计算来节省显存，参考论文《Training Deep Nets with Sublinear Memory Cost》。

yuanming-hu/taichi_mpm 带有切割和耦合（CPIC）的高性能MLS-MPM（基于移动最小二乘法的物质点法）求解器

pytorch/opacus Opacus是一个库，可以使用不同的隐私训练PyTorch模型。

pycaret/pycaret Python中的开源，低代码机器学习库

thuml/Transfer-Learning-Library 用于迁移学习的开源且文档齐全的库。它基于具有高性能和友好API的纯PyTorch。当前支持的算法包括：领域对抗神经网络（DANN）深度适应网络（DAN）联合适应网络（JAN）条件域对抗网络（CDAN）最大分类器差异（MCD）Margin Disparity Discrepancy 保证金差异（MDD）

FedML-AI/FedML 面向研究的联邦学习库。支持分布式计算，移动/IoT设备训练和模拟

bytedance/fedlearner 字节开源联邦机器学习平台Fedlearner.采用的是一套云原生的部署方案。数据存放在HDFS，用MySQL存储系统数据。通过Kubernetes管理和拉起任务。每个Fedlearner的训练任务需要参与双方同时拉起K8S任务，通过Master节点统一管理，Worker建实现通信。以推荐广告业务为例，联邦机器学习平台的广告主和平台方应该各自管理一套模型展示服务和模型训练服务。

mit-han-lab/mcunet IoT硬件上精简的深度学习库 Tiny Deep Learning on IoT Devices


### 参数优化

hyperopt/hyperopt 分布式超参数优化

optuna/optuna 超参数优化框架https://optuna.org

WillKoehrsen/hyperparameter-optimization 超参数优化

HDI-Project/BTB Bayesian Tuning and Bandits，auto-tuning系统的一个简单、可扩展的后端系统。

scikit-optimize/scikit-optimize 一个简单高效的库，可最大限度地减少（非常）昂贵且嘈杂的黑盒功能。它实现了几种基于顺序模型优化的方法。

automl/SMAC3 基于序列模型的算法配置 优化任意算法的参数

CMA-ES/pycma 基于CMA-ES 协方差矩阵的自适应策略的Py实现和一些相关的数值优化工具。

SheffieldML/GPyOpt 使用GPy进行高斯过程优化

pytorch/botorch PyTorch中的贝叶斯优化

JasperSnoek/spearmint 机器学习算法的实用贝叶斯优化

facebookresearch/nevergrad 用于执行无梯度优化的Python工具箱

Yelp/MOE 用于现实世界的指标优化的全局黑匣子优化引擎。

fmfn/BayesianOptimization 具有高斯过程的全局优化的Python实现。

dragonfly/dragonfly  用于可扩展的贝叶斯优化

音调：可伸缩超参数调整

ray-project/ray Tune可伸缩超参数调整

keras-team/keras-tuner keras的超参数调整




### 梯度提升

AugBoost 梯度提升

DeepGBM 梯度提升

CatBoost 基于梯度提升对决策树的机器学习方法

GBDT-PL/GBDT-PL 梯度提升

mesalock-linux/gbdt-rs 梯度提升

Xtra-Computing/thundergbm 梯度提升

dmlc/xgboost 梯度提升


### 分布式机器学习

horovod/horovod 分布式训练框架

dask/dask  提供大规模性能 高级并行性

Qihoo360/XLearning

sql-machine-learning/elasticdl

kubeflow/kubeflow

alibaba/euler

Angel-ML/angel

ray-project/ray 快速简单的框架，用于构建和运行分布式应用程序。

Alink 基于Flink的通用算法平台

kakaobrain/torchgpipe pytorch的可扩展的管道并行性库，可以有效地训练大型的，消耗内存的模型。

tensorflow/mesh 简化模型并行化 Mesh TensorFlow: Model Parallelism Made Easier

microsoft/DeepSpeed 一个深度学习优化库，它使分布式训练变得容易，高效和有效。

sql-machine-learning/elasticdl Kubernetes原生的深度学习框架。ElasticDL是一个基于TensorFlow 2.0的Kubernetes原生深度学习框架，支持容错和弹性调度。

uber/fiber 简化AI的分布式计算 该项目是实验性的，API不稳定。

petuum/adaptdl 资源自适应深度学习（DL）培训和调度框架。AdaptDL的目标是使分布式DL在动态资源环境（如共享集群和云）中变得轻松高效。

learning-at-home/hivemind 一个用于在互联网上训练大型神经网络的库

## 图数据库 图算法

Tencent/plato

dgraph-io/dgraph

hugegraph/hugegraph

vtraag/leidenalg

erikbern/ann-benchmarks 最邻近搜索

vesoft-inc/nebula 分布式、可扩展、闪电般的图形数据库

milvus-io/milvus 大规模特征向量的最快相似度搜索引擎 基于Faiss、Annoy等比较成熟的开源库，并针对性做了定制，支持结构化查询、多模查询等业界比较急需的功能；Milvus支持cpu、gpu、arm等多种类型的处理器；同时使用mysql存储元数据，并且在共享存储的支持下，Milvus可以支持分布式部署。

vearch/vearch 用于嵌入式向量高效相似性搜索的分布式系统

dgraph-io/dgraph The Only Native GraphQL Database With A Graph Backend.

vesoft-inc/nebula 开放源代码图数据库，能够托管具有数十亿个顶点（节点）和数万亿条边（具有几毫秒的延迟）的超大规模图。

## 大数据
Qihoo360/Quicksql 体系结构图可帮助您更轻松地访问 Quicksql

seata/seata 简单可扩展的自主事务体系结构

apache/incubator-shardingsphere 分布式数据库中间件生态圈

Tencent/wwsearch wwsearch是企业微信后台自研的全文检索引擎

apache/airflow 一个以编程方式编写，安排和监视工作流的平台

apache/shardingsphere Distributed database middleware 分布式数据库中间件

opencurve/curve 网易自主设计研发的高性能、高可用、高可靠分布式存储系统，具有非常良好的扩展性。

ClickHouse/ClickHouse 一个开源列式数据库系统，允许实时生成数据分析报告。

## 图神经网络GNN
* 图机器学习库
 * stellargraph/stellargraph 星际图机器学习库
 * Deep Graph Library (DGL)
 * alibaba/euler 分布式图深度学习框架。
 * facebookresearch/PyTorch-BigGraph 从大型图形结构化数据生成嵌入
 * rusty1s/pytorch_geometric 用于PyTorch的深度图学习扩展库
 * shenweichen/GraphNeuralNetwork 图神经网络的实现和实验，gcn\graphsage\gat等。
 * THUDM/cogdl 图形表示学习工具包，实现的模型，非GNN基线:如Deepwalk，LINE，NetMF，GNN基线:如GCN，GAT，GraphSAGE
 * imsheridan/CogDL-TensorFlow 图表示学习工具包，使研究人员和开发人员可以轻松地训练和比较基线或自定义模型，以进行节点分类，链接预测和其他图任务。它提供了许多流行模型的实现，包括：非GNN基准，例如Deepwalk，LINE，NetMF；GNN基准，例如GCN，GAT，GraphSAGE。
 * CrawlScript/tf_geometric 高效友好的图神经网络库 节点分类:图卷积网络（GCN）、多头图注意力网络（GAT），链接预测：平均池、SAGPooling，图分类：图形自动编码器（GAE）
 * alibaba/graph-learn 旨在简化图神经网络应用的框架。从实际生产案例中提取解决方案。已在推荐，反作弊和知识图系统上得到应用和验证。
 * BUPT-GAMMA/OpenHINE 异构信息网络嵌入（OpenHINE）的开源工具包。实现的模型包括：DHNE，HAN，HeGAN，HERec，HIN2vec，Metapath2vec，MetaGraph2vec，RHINE。
 * PaddlePaddle/PGL 基于PaddlePaddle的高效灵活的图学习框架
 * THUDM/cogdl 由清华大学计算机系知识工程实验室（KEG）开发的基于图的深度学习的研究工具，基于Python语言和Pytorch库。

ASTGCN 基于注意的时空图卷积网络，用于交通流量预测

danielzuegner/robust-gcn

benedekrozemberczki/ClusterGCN

zhiyongc/Graph_Convolutional_LSTM

Jiakui/awesome-gcn 该存储库用于收集GCN，GAT（图形关注）相关资源。

tkipf/gae

peter14121/intentgc-models 意图gc模型

PetarV-/GAT Graph Attention Networks 

GraphVite 高速、大规模图嵌入

williamleif/GraphSAGE

GeniePath-pytorch

xiangwang1223/neural_graph_collaborative_filtering

tkipf/gae Graph Auto-Encoders

tkipf/gcn

microsoft/gated-graph-neural-network-samples

deepmind/graph_nets 在Tensorflow中构建图网

woojeongjin/dynamic-KG 嵌入动态知识图

awslabs/dgl-ke 高性能，易于使用且可扩展的软件包，用于学习大规模知识图嵌入。

leoribeiro/struc2vec

shenweichen/GraphEmbedding

thunlp/OpenKE

Jhy1993/HAN 异构图注意力网络，遵循经典的异质图神经网络架构(节点级别聚合与语义级别聚合)，为了更好的实现层次聚合函数，HAN利用语义级别注意力和节点级别注意力来同时学习元路径与节点邻居的重要性,并通过相应地聚合操作得到最终的节点表示。

thunlp/OpenNE

tkipf/keras-gcn

aditya-grover/node2vec

thunlp/Fast-TransX

thunlp/TensorFlow-TransX

Wentao-Xu/SEEK 知识图谱嵌入框架

thunlp/KB2E

RLgraph 用于深度强化学习的模块化计算图

hwwang55/RippleNet 将知识图谱作为额外信息，融入到CTR/Top-K推荐

THUDM/cogdl 用于图形表示学习的广泛研究平台

klicperajo/ppnp 预测然后传播：图形神经网络满足个性化PageRank

inyeoplee77/SAGPool Self-Attention Graph Pooling torch自我注意力图池化

autoliuweijie/K-BERT Enabling Language Representation with Knowledge Graph ，已被AAAI2020所录取，是较早的考虑将知识图谱中的边关系引入预训练模型的论文。论文链接：https://arxiv.org/pdf/1909.07606v1.pdf 主要通过修改Transformer中的attention机制，通过特殊的mask方法将知识图谱中的相关边考虑到编码过程中，进而增强预训练模型的效果。

THU-KEG/KEPLER 主要通过添加类似于TransE的预训练机制来增强对应文本的表示，进而增强预训练模型在一些知识图谱有关任务的效果。

txsun1997/CoLAKE 使用知识图谱以增强预训练模型的效果 首先将上下文看作全连接图，并根据句子中的实体在KG上抽取子图，通过两个图中共现的实体将全连接图和KG子图融合起来；最终本文将文本上下文和知识上下文一起用MLM进行预训练，将mask的范围推广到word、entity和relation；为训练该模型，本文采用cpu-gpu混合训练策略结合负采样机制减少训练时间；最终本文提出的方法在知识图谱补全和若干NLP任务上均带来了增益。然后本文将该图转化为序列，使用Transformer进行预训练，并在训练时采用特殊的type embedding来表示实体、词语与其他子图信息，

Malllabiisc/CompGCN 针对多关系有向图的图神经网络

graphdml-uiuc-jlu/geom-gcn 几何图卷积网络 将节点映射为连续空间的一个向量（graph embedding），在隐空间查找邻居并进行聚合。

EstelleHuang666/gnn_hierarchical_pooling Hierarchical Graph Representation Learning 构建了一个多层次的、节点可微分的聚合 GNN 网络。在每一层中，完成信息的抽取，并将当前的图聚合为一个更粗粒度的图，供下一层使用。

limaosen0/Variational-Graph-Auto-Encoders 可变图自动编码器 链接预测

animutomo/gcmc Graph Convolution Matrix Completion 解决推荐系统中 矩阵补全 matrix completion 问题，并引入 side information（节点的额外信息）提升预测效果。

Ruiqi-Hu/ARGA 对抗正则化图自动编码器 Adversarially Regularized Graph Autoencoder，可用于图卷积的链路预测。进化路线 GAE -> VGAE -> ARGA 

brxx122/HeterSumGraph 用于提取文档摘要的异构图神经网络

chuxuzhang/KDD2019_HetGNN KDD2019论文中HetGNN的代码：异构图神经网络 用了LSTM作为来聚合某种关系下的节点邻居并更新节点表示。这里的邻居选择也有所不同：通过random walk with restart来选择固定数量的邻居。

TAMU-VITA/L2-GCN GCN高效分层训练框架

safe-graph/DGFraud 基于深度图的工具箱，用于欺诈检测

safe-graph/graph-fraud-detection-papers 基于图的欺诈检测论文和资源

xiangwang1223/neural_graph_collaborative_filtering 神经图协同过滤（NGCF）是一种基于图神经网络的新推荐框架，通过执行嵌入传播，在用户项二部图中以高阶连通性的形式对协同信号进行显式编码。

Googlebaba/KDD2019-MEIRec 基于异质图神经网络的用户意图推荐

mori97/JKNet-dgl 跳跃知识网络的dgl实现

lanyunshi/Multi-hopComplexKBQA 查询图生成，用于回答知识库中的多跳复杂问题.提出了一种改进的分阶段查询图生成方法，该方法具有更灵活的生成查询图的方式。在查询图生成的每一步，包含三种预定义的操作：扩展、连接、聚合。

nju-websoft/SPARQA 基于知识库的问题解答,提出了一种新颖的骨架语法来表示一个复杂问题的高级结构。骨架语法本质上是依赖语法的一个选定子集，用于专门表示复杂问题的高级结构。这种专用的粗粒度表示形式由于其简单性而可能具有准确的解析算法，有助于提高下游细粒度语义解析的准确性。

malllabiisc/EmbedKGQA 使用知识库嵌入改进知识图上的多跳问题回答

jwzhanggy/Graph-Bert 学习图形表示只需要注意力机制

aister2020/KDDCUP_2020_AutoGraph_1st_Place KDD KDD CUP 2020自动图形表示学习：第一名解决方案。实现了四种不同的模型GCN、GAT、GraphSage、TAGConv

ChandlerBang/Pro-GNN 鲁棒图神经网络的图结构学习，抗严重干扰。

THUDM/GCC Graph Contrastive Coding for Graph Neural Network Pre-Training 用于图形神经网络预训练的图形对比编码，下游任务：节点分类、图分类
、相似性搜索。

snap-stanford/distance-encoding 距离编码-为结构表示学习设计更强大的GNN，提出了一类与结构相关的特征，称为距离编码(Distance Encoding，DE)，以帮助 GNN 以比 1-WL test 更严格的表达能力来表示任意大小的节点集。

acbull/pyHGT Heterogeneous Graph Transformer 异构图Transformer

acbull/GPT-GNN Generative Pre-Training of Graph Neural Networks 图神经网络的生成式预训练。在预处理阶段，算法会首先随机地遮盖掉图中的一些边和点，利用生成模型来生成（预测）这些边的存在和节点的属性。模型的损失函数会使得预测的结果尽量接近真实的网络结构。这样的话，在GPT-GNN训练完成后，其内部的图神经网络层就可以被拿出来进行调优。

megvii-research/DPGN DPGN: Distribution Propagation Graph Network for Few-shot Learning 分布传播图网络的小样本学习

jwzhanggy/Graph-Bert 仅基于Attention 机制而不依赖任何类卷积或聚合操作即可学习图的表示，并且完全不考虑节点之间的连接信息。通过将原始图分解为以每个节点为中心的多个子图来学习每个节点的表征信息，这不仅能解决图模型的预训练问题，还能通过并行处理还提高效率。

cdjhz/multigen Language Generation with Multi-hop Reasoning on Commonsense Knowledge Graph 基于常识知识图的多跳推理语言生成 本研究关注一类条件文本生成任务，即给定输入源文本X，目标是生成一段目标文本 Y。研究员们额外增加了一个知识图谱 G=(V,E) 的输入为模型在生成时提供常识知识的信息。

LeiBAI/AGCRN 端到端的流量预测模型-自适应图卷积递归网络（AGCRN）。AGCRN可以捕获流量序列中特定于节点的细粒度空间和时间相关性，并通过嵌入DAGG来统一修订GCN中的节点嵌入。这样，训练AGCRN可以针对每个交通系列源（例如，用于交通速度/流量的道路，用于乘客需求的车站/区域）产生有意义的节点表示向量。学习的节点表示包含有关道路/区域的有价值的信息，并且可以潜在地应用于其他任务。

THUDM/GRAND Graph Random Neural Network (GRAND)，一种用于图半监督学习的新型图神经网络框架。在模型架构上，GRAND 提出了一种简单有效的图数据增强方法 Random Propagation，用来增强模型鲁棒性及减轻过平滑。基于 Random Propagation，GRAND 在优化过程中使用一致性正则（Consistency Regularization）来增强模型的泛化性，即除了优化标签节点的 cross-entropy loss 之外，还会优化模型在无标签节点的多次数据增强的预测一致性。

snap-stanford/gib 图信息瓶颈 (GIB)。研究者基于该原则构建了两个 GNN 模型：GIB-Cat 和 GIB-Bern，二者在抵御对抗攻击时取得了优异的性能。 图信息Bottleneck打造图最优表示->避免过拟合，并具备稳健性

HLTCHKUST/ke-dialogue 提出了一种将任意大小的知识库直接嵌入到模型参数中的方法

JanKalo/KnowlyBERT 提出了一种混合的语言知识模型查询系统，该系统使用语言模型来应对现实世界中知识图谱的不完整性问题。作为KnowlyBERT的输入，用户可以向系统提出以实体为中心的SPARQL查询。首先，查询语言模型（a）；然后，对不完整的知识图谱进行查询，并获得结果（b）；另外SPARQL查询被翻译成多种自然语言语句，这些语言语句在“关系模板生成”步骤中由语言模型完成；语言模型返回多个单词列表以及每个单词（c）的置信度值；然后将这些列表合并为一个列表（d），并根据知识图谱类型信息（e）使用我们的语义过滤步骤进行过滤。此外，执行阈值处理，削减不相关的结果（f）；最后，将语言模型和知识图谱的结果合并（g）并返回给用户。

CUAI/CorrectAndSmooth 标签信息 + 简单模型 该方法性能提升的主要来源是直接使用标签进行预测。与 GNN 或其他 SOTA 解决方案相比，本文中的 C&S 模型需要的参数量往往要少得多。在很多标准直推式节点分类（transductive node classification）基准上，该方法超过或媲美当前最优 GNN 的性能。

TUM-DAML/pprgo_pytorch 在一个包含1240万个节点，17300万条边组成的大规模图上，PPRGo只花了不到2分钟就给图上所有节点分了类，更夸张的是，这2分钟还是包括了预处理、训练、预测的全流程时间. PPRGo先用每个节点的本地特征学习出每个节点的本地embedding，再用PPR矩阵完成本地embedding在图上的传递与聚合

YimiAChack/GraphSTONE Graph Structural-topic Neural Network 图结构主题神经网络 本文类比自然语言处理中的相关概念，借助主题模型学习图的结构信息。


## 强化学习 Reinforcement Learning

ray-project/ray 快速简单的框架，用于构建和运行分布式应用程序。

astooke/rlpyt

Generative Adversarial User Model

dennybritz/reinforcement-learning

keiohta/tf2rl

rlgraph/rlgraph

deepmind/trfl

Ceruleanacg/Personae

dgriff777/a3c_continuous

google/dopamine

keras-rl/keras-rl

openai/gym

georgezouq/awesome-deep-reinforcement-learning-in-finance 金融市场上使用的那些AI（RL/DL/SL/进化/遗传算法）的集合

google/brain-tokyo-workshop 世界模型 prettyNEAT

google-research/football

tensortrade-org/tensortrade 一个开源强化学习框架，用于训练，评估和部署强大的交易程序。

Baekalfen/PyBoy Game Boy emulator written in Python

google-research/batch_rl 离线强化学习

tensorflow/agents TF-Agents是TensorFlow中的强化学习库

YingtongDou/Nash-Detect 通过Nash强化学习进行鲁棒的垃圾邮件发送者检测

deepmind/acme 强化学习的研究框架，强化学习组件和代理库

huawei-noah/xingtian 刑天（XingTian）是一个组件化的库，用于开发和验证强化学习算法。它支持多种算法，包括DQN，DDPG，PPO和IMPALA等，可以在多个环境中训练代理，例如Gym，Atari，Torcs，StarCraft等。

thu-ml/tianshou 天授是基于纯PyTorch强化学习的平台。与现有的强化学习库主要基于TensorFlow，具有许多嵌套类，不友好的API或速度较慢的现有学习库不同，天守提供了快速的模块化框架和pythonic API，用于以最少的行数构建深度强化学习代理代码。

Jingliang-Duan/Distributional-Soft-Actor-Critic 一种用于连续控制任务的强化学习算法—DSAC，其优势在于减少Q值的过估计并显著改进策略的性能。证明了强化学习中引入分布式回报可显著降低Q值的过估计误差，并定量表明此误差与分布的方差呈反比关系。

tencent-ailab/TLeague 一种基于竞争性自我驱动的多智能体强化学习框架。

## 神经网络结构搜索 Neural Architecture Search

huawei-noah/CARS 华为提出基于进化算法和权值共享的神经网络结构搜索

microsoft/nni 用于自动化机器学习生命周期的开源AutoML工具包，包括功能工程，神经体系结构搜索，模型压缩和超参数调整。

awslabs/autogluon 用于深度学习的AutoML工具包 https://autogluon.mxnet.io

researchmm/CDARTS 循环可微架构搜索

xiaomi-automl/FairDARTS 消除差异化架构搜索中的不公平优势



## NLP自然语言处理
### 文本分类 + Attention机制
  * tcxdgit/cnn_multilabel_classification 基于TextCNN和Attention的多标签分类
  * ilivans/tf-rnn-attention Tensorflow实现文本分类任务的关注机制。
  * skdjfla/toutiao-text-classfication-dataset 中文文本分类数据集 共382688条，分布于15类中。

### 文本摘要/指针生成网络
  * abisee/pointer-generator 使用指针生成器网络进行汇总
  * AIKevin/Pointer_Generator_Summarizer 指针生成器网络：具有关注，指向和覆盖机制的Seq2Seq，用于抽象性摘要。 tensorflow 2.0
  * kjc6723/seq2seq_Pointer_Generator_Summarizer 中文会话中生成摘要总结的项目  tensorflow 2.0
  * steph1793/Pointer_Transformer_Generator tensorflow 2.0
  * magic282/NeuSum 通过共同学习评分和选择句子进行神经文本摘要
  * dmmiller612/bert-extractive-summarizer BERT易于使用的提取文本摘要

### 文本相似度
  * UKPLab/sentence-transformers 句子转换器：使用BERT / RoBERTa / XLM-RoBERTa＆Co.和PyTorch的多语言句子嵌入
  * terrifyzhao/text_matching 常用文本匹配模型tf版本，数据集为QA_corpus，持续更新中
  * Brokenwind/BertSimilarity 基于Google的BERT模型来进行语义相似度计算。
  * wuba/qa_match 58同城推出的一款基于深度学习的轻量级问答匹配工具，它融合领域识别与意图识别，对问答意图进行精确理解。

### Transformer优化
  * ml-jku/hopfield-layers NLP 领域里大热的 Transformer，其网络更新规则其实是和 Hopfield 网络在连续状态下是相同的。Transformer 中的这种注意力机制其实等价于扩展到连续状态的 modern Hopfield 网络中的更新规则。论文作者来自奥地利林茨大学、挪威奥斯陆大学等机构，与 Jürgen Schmidhuber 合著 LSTM 的 Sepp Hochreiter 也是作者之一。
  * laiguokun/Funnel-Transformer Transformer优化，一种新的自我注意模型，可以将隐藏状态的序列逐渐压缩为较短的状态，从而降低了计算成本。
  * mit-han-lab/hardware-aware-transformers 用于高效自然语言处理的硬件感知型Transformers.实现高达3倍的加速和3.7倍的较小模型尺寸，而不会降低性能。
  * mit-han-lab/lite-transformer 具有长距离短距离注意的Lite transformer
  * DeBERTa：注意力分散的增强解码的BERT，使用两种新颖的技术改进了BERT和RoBERTa模型，显着提高了模型预训练的效率和下游任务的性能。
  * allenai/longformer 用于长文档的类似BERT的模型
  * Tencent/TurboTransformers a fast and user-friendly runtime for transformer inference on CPU and GPU
  * idiap/fast-transformers Pytorch library for fast transformer implementations
  * * bytedance/lightseq 高效的序列处理与生成库，提供 Bert, GPT, Transformer，beam search, diverse beam search, topp/topk sampling
  * Big Bird 稀疏注意力机 随机注意力机制+局部注意力机制+全局注意力机制 PurdueCAM2Project/TensorFlowModelGardeners/official/nlp/projects/bigbird/
  * lucidrains/performer-pytorch 使用一个高效的线性广义注意力框架（generalized attention framework），允许基于不同相似性度量（核）的一类广泛的注意力机制。该框架通过谷歌的新算法 FAVOR+（ Fast Attention Via Positive Orthogonal Random Features）来实现，后者能够提供注意力机制的可扩展低方差、无偏估计，这可以通过随机特征图分解（常规 softmax-attention）来表达。该方法在保持线性空间和时间复杂度的同时准确率也很有保证，也可以应用到独立的 softmax 运算。此外，该方法还可以和可逆层等其他技术进行互操作。google-research/google-research/tree/master/performer
  * microsoft/fastformers 实现Transformers在CPU上223倍的推理加速 它能对基于Transformer的模型在各种NLU任务上实现高效的推理时间性能。论文FastFormers的作者表明，利用知识蒸馏、结构化剪枝和数值优化可以大幅提高推理效率。我们表明，这种改进可以达到200倍的加速，并在22倍的能耗下节省超过200倍的推理成本。
  * mit-han-lab/lite-transformer 轻量级Transformer，注意力长短搭配 长依赖和短依赖的剥离，并引入卷积来捕捉短依赖，总体思想和Transformer之自适应宽度注意力有点类似。这篇文章中发现低层次上的注意力都比较短，层次越高，注意力的所关注的依赖越长。
  * google-research/text-to-text-transfer-transformer Text-To-Text Transfer Transformer T5的理念就是“万事皆可 Seq2Seq”，它使用了标准的 Encoder-Decoder 模型，并且构建了无监督/有监督的文本生成预训练任务，最终将效果推向了一个新高度。
  * google-research/multilingual-t5 T5 的多国语言版
  * bojone/t5_in_bert4keras 在keras中使用T5模型 , 用 mT5 small 版本 finetune 出来的 CSL 标题生成模型，BLEU 指标能持平基于 WoBERT 的 UniLM 模型，并且解码速度快 130%；而用 mT5  base 版本 finetune 出来的 CSL 标题生成模型，指标能超过基于 WoBERT 的 UniLM 模型 1% 以上，并且解码速度也能快 60%。


### BERT优化
  * google-research/bert Bidirectional Encoder Representations from Transformers 来自Transformers的双向编码器表示法
  * google-research/ALBERT 用于语言表达自我监督学习的Lite BERT
  * bojone/bert-of-theseus BERT 模型压缩方法 ,theseus(忒修斯之船 如果忒修斯的船上的木头被  逐渐替换，直到所有的木头都不是原来的木头，那这艘船还是原来的那艘船吗？),将原始大模型切分为多个大模块，固定大模型权重，训练时随机替换为小模块,充分训练后，将小模型继续微调。
  * brightmart/albert_zh 使用TensorFlow 进行自我监督学习语言表示的Lite Bert的实现预训练的汉语模型
  * bert4keras 更清晰、更轻量级的keras版bert
  * huawei-noah/Pretrained-Language-Model 华为诺亚方舟实验室开发的预训练语言模型及其相关优化技术NEZHA是一种经过预训练的中文语言模型，可以在多项中文NLP任务上实现最先进的性能TinyBERT是一种压缩的BERT模型，推理时可缩小7.5倍，加快9.4倍
  * Lisennlp/TinyBert 基于华为的TinyBert进行修改的，简化了数据读取的过程，方便我们利用自己的数据进行读取操作。
  * epfml/collaborative-attention 整合多头注意力,任何经过预训练的注意力层重新配置为协作注意力层。
  * thunlp/ERNIE 用知识图谱增强 BERT 的预训练效果 
    * 1) 对于抽取并编码的知识信息，研究者首先识别文本中的命名实体，然后将这些提到的实体与知识图谱中的实体进行匹配。研究者并不直接使用 KG 中基于图的事实，相反他们通过知识嵌入算法（例如 TransE）编码 KG 的图结构，并将多信息实体嵌入作为 ERNIE 的输入。基于文本和知识图谱的对齐，ERNIE 将知识模块的实体表征整合到语义模块的隐藏层中。
    * 2) 与 BERT 类似，研究者采用了带 Mask 的语言模型，以及预测下一句文本作为预训练目标。除此之外，为了更好地融合文本和知识特征，研究者设计了一种新型预训练目标，即随机 Mask 掉一些对齐了输入文本的命名实体，并要求模型从知识图谱中选择合适的实体以完成对齐。
 * ZhuiyiTechnology/WoBERT 以词为基本单位的中文BERT（Word-based BERT）
 * autoliuweijie/FastBERT FastBERT：具有自适应推断时间的自蒸馏BERT pip install fastbert
 * alexa/bort 论文 Optimal Subarchitecture Extraction for BERT. “ BERT的最佳子体系结构提取”的代码。Bort是用于BERT架构的最佳子集，它是通过对神经架构搜索应用完全多项式时间近似方案（FPTAS）提取的。 Bort的有效（即不计算嵌入层）大小是原始BERT大型体系结构的5.5％，是净大小的16％。它在CPU上也比基于BERT的速度快7.9倍，并且比体系结构的其他压缩变体和某些非压缩变体性能更好。与多个公共自然语言理解（NLU）基准上的BERT-large相比，它的平均性能提高了0.3％至31％。
 * ymcui/MacBERT MacBERT是经过改进的BERT，具有新颖的MLM作为校正预训练任务，从而减轻了预训练和微调的差异。
 * valuesimplex/FinBERT 基于 BERT 架构的金融领域预训练语言模型
 * yitu-opensource/ConvBert ConvBERT，通过全新的注意力模块，仅用 1/10 的训练时间和 1/6 的参数就获得了跟 BERT 模型一样的精度。依图研发团队从模型结构本身的冗余出发，提出了一种基于跨度的动态卷积操作，并基于此提出了 ConvBERT 模型。
 * wtma/CharBERT 字符敏感的预训练语言模型 通过结合字符级别和词级别的信息实现了更为全面的输入编码，同时，结合 RNN 和 CNN 的优势，基本上 CNN，RNN，Transformer 都使用上了，体现了新老研究成果的结合在一定程度上能进一步提升方法的性能。
 * bohanli/BERT-flow 基于流式生成模型，将BERT的表示可逆地映射到一个均匀的空间，文本表示新SOTA。
 * Sleepychord/CogLTX 将BERT应用于长文本 CogLTX 遵循一种特别简单直观的范式，即 抽取关键的句子 => 通过 BERT 得到答案 这样的两步流程。

huseinzol05/NLP-Models-Tensorflow 抽象总结 聊天机器人依赖解析器 实体标记 提取摘要 发电机 语言检测 神经机器翻译 光学字符识别 POS标签 问题答案 句子对 语音转文字 拼写校正 小队问题答案 抽干 文字扩充 文字分类 文字相似度 文字转语音 主题生成器 主题建模 无监督提取摘要 矢量化器 老少少的声码器 可视化 注意Attention

CyberZHG/keras-xlnet XLNet的非官方实现。

ymcui/Chinese-XLNet 面向中文的XLNet预训练模型

bojone/attention  Attention机制的实现tensorflow/keras

ouyanghuiyu/chineseocr_lite 超轻量级中文ocr

LeeSureman/Flat-Lattice-Transformer 中文NER 基于Transformer设计了一种巧妙position encoding来融合Lattice结构，可以无损的引入词汇信息。基于Transformer融合了词汇信息的动态结构，支持并行化计算，可以大幅提升推断速度。

thu-coai/CrossWOZ 大规模的中文跨域任务导向对话数据集.它包含5个领域的6K对话会话和102K语音，包括酒店，餐厅，景点，地铁和出租车。

425776024/nlpcda 一键中文数据增强工具,支持：1.随机实体替换 2.近义词 3.近义近音字替换 4.随机字删除 5.NER类 BIO 数据增强 6.随机置换邻近的字  7.百度中英翻译互转实现的增强  8.中文等价字替换

wac81/textda Python3中文文本的数据增强

zhanlaoban/EDA_NLP_for_Chinese 适合中文语料的数据增强EDA的实现

goto456/stopwords 中文常用停用词表

chatopera/Synonyms 用于自然语言处理和理解的中文同义词。

ShomyLiu/Neu-Review-Rec Pytorch的基于评论文本的深度推荐系统模型库。DeepCoNN(WSDM'17)、D-Attn(RecSys'17)、ANR(CIKM'18)、NARRE(WWW'18)、MPCN(KDD'18)、TARMF(WWW'18)、CARL(TOIS'19)、CARP(SIGIR'19)、DAML(KDD'19)

ShannonAI/service-streamer 服务流媒体BERT服务,每秒处理1400个句子的BERT服务.

squareRoot3/Target-Guided-Conversation 目标指导的开放域对话,在开放域的聊天中目标引导.

weizhepei/CasRel 一种用于关系三重提取的新颖级联二进制标记框架.

qiufengyuyi/sequence_tagging 使用bilstm-crf，bert等方法进行序列标记任务

microsoft/unilm UniLM-NLP及更高版本的统一语言模型预训练

airaria/TextBrewer 基于PyTorch实现的NLP任务知识蒸馏工具包，适用于多种模型结构，支持自由组合各种蒸馏策略，并且在文本分类、阅读理解、序列标注等典型NLP任务上均能获得满意的效果。 

czhang99/SynonymNet 基于多个上下文双向匹配的同义实体发现

PRADO 用于文档分类的投影注意网络 性能媲美BERT，但参数量仅为1/300 tensorflow/models/tree/master/research/sequence_projection

rikdz/GraphWriter 基于图Transformer从知识图谱中生成文本

DC-BERT: Decoupling Question and Document for Efficient Contextual Encoding 双重 BERT 模型的解耦上下文编码框架 shawroad/NLP_pytorch_project/Text_Ranking/DC_Bert_Ranking/

fushengwuyu/chinese_spelling_correction 中文文本纠错模型：bert语言模型+字音字形相似度 、MLM、seq2seq

stanford-futuredata/ColBERT ColBERT: 基于上下文（contextualized）的后期交互的排序模型 Efficient and Effective Passage Search via Contextualized Late Interaction over BERT 兼顾匹配的效率和doc中的上下文信息

ymcui/Chinese-ELECTRA 中文ELECTRA预训练模型 其中ELECTRA-small模型可与BERT-base甚至其他同等规模的模型相媲美，而参数量仅为BERT-base的1/10

salesforce/pytorch-qrnn 准循环神经网络Quasi-Recurrent Neural Network,基于使用实例可以比高度优化的 NVIDIA cuDNN LSTM 实现2到17倍快

ChenghaoMou/pytorch-pQRNN pQRNN 结合一个简单的映射和一个quasi-RNN编码器来进行快速并行处理。pQRNN模型表明这种新的体系结构几乎可以达到BERT级的性能，尽管只使用1/300的参数量和有监督的数据。

alibaba/EasyTransfer 自然语言处理的迁移学习工具。主要特性：预训练语言模型工具，丰富且高质量的预训练模型库 BERT, ALBERT, RoBERTa, T5, etc,丰富且易用的NLP应用 如文本匹配、分本分类、机器阅读理解MRC，自动化的知识蒸馏，易用且高效的分布式训练。

RUCAIBox/TG-ReDial 一个电影领域的对话推荐数据集TG-ReDial (Recommendation through Topic-Guided Dialog)。它包含1万个完整的对话和近13万条语句，加入了话题线索以实现将用户引导至推荐场景这一语义的自然转移，并且采用半自动的方式构建，保留了用户真实的个性化信息（如交互历史，偏好主题），使得人工标注过程更加合理可控。

RUCAIBox/TG_CRS_Code TG-ReDial相应的推荐、回复生成、主题预测功能实现。

loujie0822/DeepIE DeepIE： 基于深度学习的信息抽取技术,实体抽取\实体关系联合抽取\属性抽取\实体链接/标准化\事件抽取\摘要抽取

Qznan/QizNLP 快速运行分类、序列标注、匹配、生成等NLP任务的Tensorflow框架 (中文 NLP 支持分布式）

131250208/TPlinker-joint-extraction 联合抽取模型 实体关系联合抽取标注方案

toizzy/tilt-transfer 运行TILT迁移学习实验的代码 让语言模型先在乐谱上进行训练，再在自然语言上训练可以有效的提升语言模型的性能。

nju-websoft/NEST 输入知识图谱的基于联合编码的弱监督神经实体摘要方法

XiaoMi/MiNLP/tree/main/minlp-tokenizer 小米 AI 实验室 NLP 团队开发的小米自然语言处理平台 MiNLP 现已开源了中文分词功能



## 推荐系统

shenweichen/DeepCTR

ChenglongChen/tensorflow-DeepFM

cheungdaven/DeepRec

lyst/lightfm

tensorflow/recommenders TensorFlow Recommenders is a library for building recommender system models using TensorFlow.

RUCAIBox/RecBole 统一，全面，高效的推荐库，包括：AFM,AutoInt,DCN,DeepFM,DSSM,FFM,FM,FNN,FwFM,LR,NFM,PNN,WideDeep,xDeepFM,BPR,ConvNCF,DGCF,DMF,FISM,GCMC,ItemKNN,LightGCN,NAIS,NeuMF,NGCF,Pop,SpectralCF,CFKG,CKE,KGAT,KGCN,KGNNLS,KTUP,MKR,RippleNet,BERT4Rec,Caser,DIN,FDSA,FPMC,GCSAN,GRU4Rec,GRU4RecF,GRU4RecKG,KSR,NARM,NextItNet,S3Rec,SASRec,SASRecF,SRGNN,STAMP,TransRec

oywtece/dstn

shenweichen/DSIN

facebookresearch/dlrm 深度学习推荐模型（DLRM）的实现

vze92/DMR Deep Match to Rank Model for Personalized Click-Through Rate Prediction DMR：Matching和Ranking相结合的点击率预估模型

kang205/SASRec 源于Transformer的基于自注意力的序列推荐模型

shichence/AutoInt 使用Multi-Head self-Attention进行自动的特征提取

xiangwang1223/neural_graph_collaborative_filtering 神经图协同过滤

UIC-Paper/MIMN 点击率预测的长序列用户行为建模的实践

motefly/DeepGBM 结合了 GBDT 和神经网络的优点，在有效保留在线更新能力的同时，还能充分利用类别特征和数值特征。DeepGBM 由两大块组成，CatNN 主要侧重于利用 Embedding 技术将高维稀疏特征转为低维稠密特征，而 GBDT2NN 则利用树模型筛选出的特征作为神经网络的输入，并通过逼近树结构来进行知识蒸馏。

shenweichen/DeepMatch 用于推荐和广告的深度匹配模型库。训练模型和导出用户和项目的表示向量非常容易，可用于ANN搜索。

LeeeeoLiu/ESRM-KG 关键词生成的基于电商会话的推荐模型

zhuchenxv/AutoFIS 自动特征交互选择的点击率预测模型

pangolulu/exact-k-recommendation 解决推荐中带约束的Top-K优化问题

Scagin/NeuralLogicReasoning 神经协同推理,提出了一种新的神经逻辑推荐（NLR）框架，能够将逻辑结构和神经网络相结合，将推荐任务转化为一个逻辑推理任务。

ZiyaoGeng/Recommender-System-with-TF2.0 CTR预言论文进行复现，包括传统模型（MF，FM，FFM等），神经网络模型（WDL，DCN等）以及序列模型（DIN）。

allenjack/HGN 用矩阵分解的形式捕捉用户的长期兴趣，同时将短期兴趣进行拆分，分为group-level以及instance-level的，通过Hierarchical Gating来处理group-level的信息,item-item的乘积来捕捉商品之间的关系。

THUwangcy/ReChorus 用于Top-K推荐的通用PyTorch框架，具有隐式反馈，尤其是用于研究目的。BPR\NCF\Tensor\GRU4Rec\NARM\SASRec\TiSASRec\CFKG\SLRC\Chorus

RUCAIBox/CIKM2020-S3Rec 自我推荐学习，用于具有互信息最大化的顺序推荐

chenchongthu/SAMN 社交注意力记忆网络在推荐系统中的应用

Lancelot39/KGSF 基于知识图谱语义融合改进会话推荐系统
Improving Conversational Recommender Systems via Knowledge Graph based Semantic Fusion

DeepGraphLearning/RecommenderSystems 顺序推荐 基于维度的推荐 社交推荐

FeiSun/BERT4Rec 基于BERT的顺序推荐

ChuanyuXue/CIKM-2019-AnalytiCup 2019-CIKM挑战赛，超大规模推荐之用户兴趣高效检索赛道 冠军解决方案 ,召回阶段基于 Item CF 相似性做召回( item-item 相似性),排序阶段,最终使用了 Catboost 和 Lightgbm 建模。

zyli93/InterHAt 通过分层注意力预测可解释的点击率。

SSE-PT/SSE-PT 基于Transformer的模型,但是和SASRec类似, 效果不错,但是缺少个性化,而且没有加入基于个性化的用户embedding。为了克服这种问题,本文提出来一种个性化的Transformer(SSE-PT),该方法相较于之前的方案提升了5%。

NVIDIA/NVTabular 为特征工程、前处理提供了更快的迭代速度，同时利用异步批量加载的方法有效提高了GPU的利用率，提供更快的加载速率。Merlin推荐系统框架的模块。

NVIDIA/HugeCTR a high efficiency GPU framework designed for Click-Through-Rate (CTR) estimating training ，在Embedding lookup上做了很多优化，可以轻易的通过数据和模型并行的方式将模型扩展到TB级别，在大规模参数的背景下，这给挖掘模型能力提供了更多的想象力。同时更快的训练速度也让算法工程师能够尝试更多的网络结构，挖掘最适合所研究问题的模型。

triton-inference-server/server 面向高吞吐低延时的生产环境的框架，通过Triton做线上推理，将TensorRT作为执行后端，能够有效降低Latency，并最大化地利用GPU资源。相比于一个纯CPU的方案，两者的结合使用能够使Latency达到原先的1/18，数据吞吐量达到原先的17.6倍。

lqfarmer/GraphTR 采用了GraphSAGE+FM+Transformer多种手段，粒度上从粗到细，交叉、聚合来自不同领域的异构消息，相比于mean/max pooling、浅层FC等传统聚合方式，极大提升了模型的表达能力

guyulongcs/CIKM2020_DMT 将兴趣建模、多任务学习、偏置学习等几部分进行融合，提出了DMT模型（Deep Multifaceted Transformers）

hwwang55/DKN DKN，将知识图表示融入到新闻推荐中。DKN是一种基于内容的用于点击率预估的深度推荐框架。DKN的主要部分是一个多通道、单词实体对齐的知识感知卷积神经网络，KCNN，其中融入了新闻在语意层面和知识层面的表示。KCNN将单词和实体作为多通道，在卷积过程中明确保留他们之间的对齐关系。

yusanshi/NewsRecommendation NRMS NAML LSTUR DKN Hi-Fi Ark TANR

johnny12150/GCE-GNN 提出了一种全局上下文增强(global-context enhanced)的GNN网络，称为GCE-GNN。能够从两种层次来学习物品的表征，包括global-level：从所有session构成的图上进行全局的表征；以及session-level：从单个session局部item转移图上进行局部的表征；最后融合二者，并通过注意力机制形成最终的序列表征，用于序列推荐任务。

BinbinJin/SD-GAR 第一篇将生成式对抗网络（GAN）框架应用于信息检索（包括推荐系统）的研究工作。在该工作中，IRGAN 训练了一个生成器和一个判别器，其中生成器用来自适应地生成合适的负样本以帮助判别器训练；而判别器则是用来判断样本是来自用户真实的反馈还是生成器生成的样本。通过两者交替式对抗性地训练达到互相提升效果的目的。

twchen/lessr 将会话记录构建成图来建模商品之间的跳转关系的图神经网络



## 金融股票 时间序列

QUANTAXIS/QUANTAXIS 量化金融策略框架

ricequant/rqalpha 从数据获取、算法交易、回测引擎，实盘模拟，实盘交易到数据分析，为程序化交易者提供了全套解决方案

cedricporter/funcat 将同花顺、通达信、文华财经麦语言等的公式写法移植到了 Python

georgezouq/awesome-deep-reinforcement-learning-in-finance 金融市场上使用的那些AI（RL/DL/SL/进化/遗传算法）的集合

wangshub/RL-Stock 如何用深度强化学习自动炒股。

tensortrade-org/tensortrade 一个开源强化学习框架，用于训练，评估和部署强大的交易程序。

bsolomon1124/pyfinance 为投资管理和证券收益分析而构建的Python分析包。

arrigonialberto86/deepar Amazon于2017年提出的基于深度学习的时间序列预测方法

fjxmlzn/DoppelGANger 使用GAN共享网络时间序列数据：挑战，初步承诺和未解决的问题，IMC 2020（最佳论文入围）

AIStream-Peelout/flow-forecast 一个开源的深度学习时间序列预测库。包括模型：Vanilla LSTM、Full transformer、Simple Multi-Head Attention、Transformer w/a linear decoder、DA-RNN (CPU only for now)。

microsoft/qlib Qlib是一个面向AI的量化投资平台，旨在实现潜力，增强研究能力并创造AI技术在量化投资中的价值。包括多个模型。

## 虚拟化
jesseduffield/lazydocker docker 简单终端 UI

KubeOperator/KubeOperator 

rancher/k3s Lightweight Kubernetes. 5 less than k8s. https://k3s.io

docker-slim/docker-slim 请勿更改Docker容器映像中的任何内容并将其最小化30倍

silenceshell/docker_mirror 发现国内加速的docker源。

## 网络爬虫 下载
soimort/you-get youtube下载

shengqiangzhang/examples-of-web-crawlers python爬虫例子

itgoyo/Aria2  突破百度云限速合集

PanDownloadServer/Server 百度云PanDownload的个人维护版本

## 语音
JasonWei512/Tacotron-2-Chinese 中文语音合成

TensorSpeech/TensorflowTTS Tensorflow 2的实时最新语音合成

audier/DeepSpeechRecognition 基于深度学习的中文语音识别系统

athena-team/athena 基于序列到序列的语音处理引擎的开源实现

espnet/espnet  End-to-End Speech Processing Toolkit 端到端的语音处理工具箱，主要特性：kaldi风格的处理模式、ASR、TTS、语音翻译、机器翻译、语音转换、DNN框架

kan-bayashi/ParallelWaveGAN Parallel WaveGAN (+ MelGAN & Multi-band MelGAN) implementation with Pytorch 

KuangDD/zhrtvc  好用的中文语音克隆兼中文语音合成系统，包含语音编码器、语音合成器、声码器和可视化模块。 

JasonWei512/Tacotron-2-Chinese 中文语音合成

lturing/tacotronv2_wavernn_chinese tacotronV2 + wavernn 实现中文语音合成(Tensorflow + pytorch) 

JasonWei512/wavenet_vocoder  WaveNet 声码器 

deezer/spleeter 人声分离模型

## 其他
modichirag/flowpm TensorFlow中的粒子网格模拟N体宇宙学模拟

huihut/interview C/C++ 技术面试基础知识总结

barry-ran/QtScrcpy Android实时显示控制软件 

minivision-ai/photo2cartoon 人像卡通化探索项目

hugozanini/realtime-semantic-segmentation 使用TensorFlow.js实施RefineNet以在浏览器中执行实时实例分割

iPERDance/iPERCore 处理人体图像合成任务。其中包括人体运动模仿、外观转换和新视角合成等。并且，该项目的代码、数据集已开源。

facebookresearch/pifuhd 使用AI从2D图像生成人的3D高分辨率重建

LeonLok/Multi-Camera-Live-Object-Tracking 多摄像头实时目标跟踪和计数，使用YOLOv4，Deep SORT和Flask

cfzd/Ultra-Fast-Lane-Detection 论文“超快速结构感知深度车道检测”的实现

RangiLyu/nanodet NanoDet：轻量级（1.8MB）、超快速（移动端97fps）目标检测项目

kornia/kornia 基于 PyTorch 的可微分的计算机视觉 （differentiable computer vision） 开源库， 实现了：可微的基础计算机视觉算子。可微的数据增广（differentiable data augmentation）。OpenCV 和 PIL 都是不可微的，所以这些处理都只可以作为图像的预处理而无法通过观察梯度的变化来对这些算子进行优化 （gradient-based optimization）。因此，Kornia 便应运而生。

microsoft/Bringing-Old-Photos-Back-to-Life 旧照片修复

kexinhuang12345/DeepPurpose 一个基于PyTorch的工具包来解锁50多个用于药物-靶标相互作用（Drug-Target Interaction）预测的模型。DTI预测是新药研发中的一项基本任务。DeepPurpose的操作模式是像scikit-learn一样。只需几行代码，就可以利用最前沿的深度学习和药物研发模型。DeepPurpose还有一个简单的界面来做DTI预测的两个重要应用：虚拟筛选（Virtual Screening）和旧药新用（Drug Repurposing）。

bennettfeely/bennett ztext 易于实现的3D网页排版。适用于每种字体。

DaveJarvis/keenwrite 基于Java的桌面Markdown编辑器，具有实时预览，字符串插值和公式

vinayak-mehta/present 基于终端的演示工具，具有颜色和效果。

willmcgugan/rich 一个终端内富文本和美化的python库。

occlum/occlum 蚂蚁集团自研的开源可信执行环境（Trusted Execution Environments，简称 TEE） OS 系统 Occlum ,大幅降低 SGX 应用开发的门槛.机密计算（Confidential Computing）使得数据始终保持加密和强隔离状态，从而确保了用户数据的安全和隐私。

matazure/mtensor 一个tensor计算库, 支持cuda的延迟计算

fofapro/vulfocus 一个漏洞集成平台，将漏洞环境 docker 镜像，放入即可使用，开箱即用。

FangpingWan/DeepCPI 基于深度学习的化合物和蛋白质相互作用预测框架

marcopodda/fragment-based-dgm 基于片段的分子深度生成模型.作者在ZINC数据集上进行了实验，该数据由250K类药物化合物组成。为了进一步评估LFM的影响，作者还使用了Pub Chem Bio Assay(PCBA)数据集测试了模型变体，该数据集包括约440k小分子。

salesforce/provis BERTology Meets Biology: Interpreting Attention in Protein Language Models 注意力机制在蛋白质语言模型的应用

hoya012/awesome-anomaly-detection 异常检测


